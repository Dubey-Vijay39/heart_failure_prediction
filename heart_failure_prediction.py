# -*- coding: utf-8 -*-
"""heart_failure_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wc5PdHOMLptHGq_Nh6dqevPzUnWWvJXK

<h3 style='color:#ffa64d'>Context</h3>
<p>Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated <span style="color:#ffa64d;background-color:#196666;border-radius:5px;">17.9 million</span> lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.

People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.
</p>

<h3 style='color:#ffa64d'>Attribute Information</h3>

**Age:** age of the patient (years)

**Sex:** sex of the patient (M: Male, F: Female)

**ChestPainType:** chest pain type (TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic)

**RestingBP:** Resting blood pressure (mm Hg)

**Cholesterol:** serum cholesterol (mm/dl)

**FastingBS:** fasting blood sugar (1: if FastingBS > 120 mg/dl, 0: otherwise)

**RestingECG:** resting electrocardiogram results (Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria)

**MaxHR:** maximum heart rate achieved (Numeric value between 60 and 202)

**ExerciseAngina:** exercise-induced angina (Y: Yes, N: No)

**Oldpeak:** oldpeak = ST (Numeric value measured in depression)

**ST_Slope:** the slope of the peak exercise ST segment (Up: upsloping, Flat: flat, Down: downsloping)

**HeartDisease:** output class (1: heart disease, 0: Normal)

<h3 style='color:#ffa64d'>Table of content:</h3>

[1. Importing The Dependencies..](#1)

[2. Read Data from csv.](#2)

[3. Data Stucture Analysis.](#3)
* [3.1 Dataset shape and data type](#3.1)
* [3.2 Numerical data describe](#3.2)
* [3.3 Correlation between all features.](#3.3)


[4. Handling null values and outliers.](#4)
* [4.1 Null Values ](#4.1)
* [4.2 Handle if there are any outliers](#4.2)


[5. Data Visualization.](#5)

* [5.1 Box plot of all features.](#5.1)
* [5.2 Distribution of Numerical Features.](#5.2)
* [5.3 Histogram of Numerical Features.](#5.3)
* [5.4 Pair plot of Numerical Features.](#5.4)



[6. Data preproessing.](#6)
* [6.1 Checking whether data is unbalanced or not.](#6.1)
* [6.2 Handling categorical variables.](#6.2)
* [6.3 Splitting the Dataset.](#6.3)


[7. Building the Models.](#7)
* [7.1 Confusion Matrix drawing function.](#7.1)
* [7.2 Decision Tree.](#7.2)
* [7.3 Random Forest.](#7.3)
* [7.4 XGBoost.](#7.4)

## <b id='1'>1 <span style='color:#ffa64d'>|</span> Importing The Dependencies.</b>
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

"""## <b id='2'>2 <span style='color:#ffa64d'>|</span> Read Data from csv.</b> """

data = pd.read_csv("heart.csv")
data.head()

data.tail()

"""## <b id='3'>3 <span style='color:#ffa64d'>|</span> Data Stucture Analysis</b>

<p>Now we need to analyze data structure because data structure analysis plays an important role in various aspects of the modeling process, including data preprocessing, feature engineering, and model evaluation.
    
During data preprocessing, data structure analysis can help in identifying and handling missing values, outliers, and other issues that may affect the quality of the data. By analyzing the data structure, developers can determine the appropriate techniques to use for data cleaning, normalization, and transformation, which can improve the accuracy and reliability of the model.
</p>

<div id='3.1' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>3.1 | </span></span></b>Dataset shape and data type</b></p></div>
"""

data.shape

data.dtypes

data.info()

data.columns

"""<p><span style='color:#ffa64d'>Note: </span>We can see that our  data has 918 rows and 12 columns. Sex,ChestPainType,RestingECG, ExerciseAngina, and ST_Slope are object time means that are ordinal categorical data others are numbers and our target variable is HeartDisease.</p>

<div id='3.2' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>3.2 | </span></span></b>Numerical data describe</b></p></div>

<p>The describe() method in Pandas is used to get a quick statistical summary of a DataFrame. It calculates several summary statistics for each column in the DataFrame, including the count, mean, standard deviation, minimum value, 25th percentile, median (50th percentile), 75th percentile, and maximum value.</p>
"""

data.describe()

yes= data[data['HeartDisease'] == 1]. describe().T
no= data[data['HeartDisease']==0].describe().T
color= ['#ADD8E6','#E0FFFF']
fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (10,10))



plt.subplot(1,2,1)
sns.heatmap(yes[['mean']],cmap= color, annot= True, cbar= False, linecolor= 'black', linewidths= 0.4, fmt= '.2f')
plt.title('Heart Disease')

plt.subplot(1,2,2)
sns.heatmap(no[['mean', ]], annot= True,cmap= color, linewidth= 0.4, linecolor= 'black', fmt= '.2f', cbar= False)
plt.title('No heart Disease')
fig.tight_layout(pad = 2)

"""<p>
<h5 style='color:#ffa64d'>Now we take a look at what describe method shows to us.</h5>


<span style='color:#ffa64d'>Age:</span> Maximum age is 77 and the minimum age is 28, the mean is 53.51 and 75 percent of the age is below 60.

<span style='color:#ffa64d'>RestingBP:</span> Maximum value of  RestingBP is 200 and the minimum is 0 and 75 percent of the Bp is below 140.

<span style='color:#ffa64d'>Cholesterol:</span> Maximum value of  Cholesterol is 603 and the minimum is 0 and 75 percent of the Cholesterol is below 267.

<span style='color:#ffa64d'>FastingBS:</span> Maximum value of  FastingBSis is 1 and the minimum is 0 and 75 percent of the FastingBS is below 0.

<span style='color:#ffa64d'>MaxHR:</span> Maximum value of  MaxHR is  202  and the minimum is 60 and 75 percent of the MaxHR is below 156.

<span style='color:#ffa64d'>Oldpeak:</span> Maximum value of  Oldpeak is 6.2  and the minimum is -2.6 and 75 percent of the Oldpeak is below 1.5.
</p>

<div id='3.3' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>3.3 | </span></span></b>Correlation between all features.</b></p></div>

<p>Correlation is a statistical technique that measures the strength and direction of the relationship between two variables. In other words, it helps us understand how much one variable is related to another variable.

A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease. Correlation can also be neutral or zero, meaning that the variables are unrelated.

<span style='color:#ffa64d'>Positive Correlation:</span> both variables change in the same direction.
    
<span style='color:#ffa64d'>Neutral Correlation: </span>No relationship in the change of the variables.
    
<span style='color:#ffa64d'>Negative Correlation:</span> variables change in opposite directions.
</p>
"""

corr = data.corr()
plt.figure(figsize=(15,8))
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr,mask=mask,cmap='BrBG', vmin=-1, vmax= 1 , center=0, annot=True,linewidth=.5,square=False)

"""<h5 style='color:#ffa64d'>Correlation between all features with heart disease.</h5>"""

plt.figure(figsize=(15,8))

sns.heatmap(data.corr()[['HeartDisease']].sort_values(by='HeartDisease', ascending=False),cmap='BrBG', vmin=-1, vmax= 1 , center=0, annot=True,linewidth=.5,square=False)

"""<p><span style='color:#ffa64d'>Note: </span>We can see that Age, Oldpeak, RestingBP, and FastingBS all are positively correlated with heart disease, and Cholesterol, and MaxHR is negatively correlated.</p>

## <b id='4'>4 <span style='color:#ffa64d'>|</span> Handling null values and outliers.</b>

<div id='4.1' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>4.1 | </span></span></b>Null Values</b></p></div>
"""

data.isnull().sum()

"""<p><span style='color:#ffa64d'>Note: </span>We can see that there are no null values so we do not need to handle it.</p>

<div id='4.2' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>4.2 | </span></span></b>Handle if there are any outliers</b></p></div>

<p>Including outliers in data driven models could be risky. The existence of an extreme single misleading value has the potential to change the conclusion implied by the model. It is therefore important to manage that kind of risk.

There are many outlier detection algorithms available, including Z-Score, Modified Z-Score, Tukey's Method, and the standard deviation method.
    
To use the standard deviation method for detecting outliers, you would typically follow these steps:
    
<span style='color:#ffa64d'>1. </span> Calculate the mean (average) of the data set.
    
<span style='color:#ffa64d'>2. </span> Calculate the standard deviation of the data set.
    
<span style='color:#ffa64d'>3. </span> Determine a threshold for identifying outliers. One common approach is to use a multiple of the standard deviation, such as 2 or 3 standard deviations from the mean.
    
<span style='color:#ffa64d'>4. </span> Identify any observations that are more than the threshold distance from the mean as outliers.
</p>
"""

data.shape

means = data.mean()
std_devs = data.std()

# Define a threshold for identifying outliers (in this case, 2 standard deviations from the mean)
threshold = 4 * std_devs
# Identify any values that are more than the threshold distance from the mean as outliers
outliers = data[(np.abs(data - means) > threshold).any(axis=1)]
data =  data.drop(outliers.index)

data.shape

"""<h5 style='color:#ffa64d'>Histogram without outliers</h5>"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']
plt.figure(figsize=(15,12))
plt.title("Numerical Data with box plot")
for i,category in enumerate(column):
    plt.subplot(2,3,i+1)
    sns.scatterplot(data=data[category],color='#ffa64d')
    plt.title("Histogram of {} without outliers".format(category))
plt.tight_layout()

plt.show()

"""<h5 style='color:#ffa64d'>Histogram of outliers</h5>"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']
plt.figure(figsize=(15,12))
plt.title("Numerical Data with box plot")
for i,category in enumerate(column):
    plt.subplot(2,3,i+1)
    sns.scatterplot(data=outliers[category],color='#ffa64d')
    plt.title("Histogram of {} outliers".format(category))
plt.tight_layout()

plt.show()

"""## <b id='5'>5 <span style='color:#ffa64d'>|</span> Data Visualization.</b>

<div id='5.1' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>5.1 | </span></span></b>Box plot of all features.</b></p></div>
"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']
plt.figure(figsize=(15,12))
plt.title("Numerical Data with box plot")
for i,category in enumerate(column):
    plt.subplot(2,3,i+1)
    sns.boxplot(data=data[category],color='#ffa64d',orient="h")
    plt.title(category)
plt.show()

"""<div id= '5.2' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>5.2 | </span></span></b>Distribution of Numerical Features.</b></p></div>"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']
plt.figure(figsize=(15,12))
for i,category in enumerate(column):
    plt.subplot(2,3,i+1)
    sns.kdeplot(data=data, x=data[category], hue="HeartDisease", multiple="stack")
    plt.title(category)
plt.show()

"""<div id='5.3' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>5.3 | </span></span></b>Histogram of Numerical Features.</b></p></div>"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']
plt.figure(figsize=(15,12))
for i,category in enumerate(column):
    plt.subplot(2,3,i+1)
    sns.histplot(data=data, x=data[category], hue="HeartDisease", multiple="stack")
    plt.title(category)
plt.show()

"""<div id='5.4' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>5.4 | </span></span></b>Pair plot of Numerical Features.</b></p></div>"""

column=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak','HeartDisease']
sns.pairplot(data=data[column], hue="HeartDisease", corner=True, diag_kind='hist')
plt.suptitle('Pairplot: Numerical Features ' ,fontsize = 24)
plt.show()

"""## <b id='6'>6 <span style='color:#ffa64d'>|</span> Data preproessing.</b>

<div id='6.1' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>6.1 | </span></span></b>Checking whether data is unbalanced or not.</b></p></div>
"""

heart_disease = data[data.HeartDisease == 1]
normal = data[data.HeartDisease == 0]
heart_disease_prcentence = []
heart_disease_prcentence.append((len(heart_disease)/len(data))*100)
heart_disease_prcentence.append((len(normal)/len(data))*100)
heart_disease_label = ['Heart Disease','Normal']

plt.figure(figsize=(5,5))
plt.pie(heart_disease_prcentence, labels=heart_disease_label, autopct='%1.1f%%',shadow=True)

"""<p><span style='color:#ffa64d'>Note: </span>We can see that the data is balanced so we can leave it as it is.</p>

<div id='6.2' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>6.2 | </span></span></b>Handling categorical variables.</b></p></div>

We have some categorical variables available now we need to handle them. Now there is a function to take care of all categorical variables with pandas that is one hot encoding. one-hot encoding aims to transform a categorical variable with n outputs into n binary variables.
"""

data = pd.get_dummies(data)

data.head()

"""<p><span style='color:#ffa64d'>Note: </span>Now we can see that categorical variables are converted to a numerical form.</p>

<div id='6.3' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>6.3 | </span></span></b>Splitting the Dataset.</b></p></div>

We need to choose input features and also target variable.
"""

features = data.drop('HeartDisease', axis=1)
features.head()

target_variable = data['HeartDisease']
target_variable.head()

"""<p><span style='color:#ffa64d'>Splitting into train and validation set: </span>By using similar data for training and testing, we can minimize the effects of data discrepancies and better understand the characteristics of the model. After a model has been processed by using the training set, we test the model by making predictions against the test set.

So in this section, we will split our dataset into train and test datasets. We will use the function train_test_split from Scikit-learn.</p>
"""

X_train, X_val, y_train, y_val = train_test_split(features, target_variable, train_size = 0.8, random_state = 55)

print(f'train samples: {len(X_train)}')
print(f'validation samples: {len(X_val)}')

"""## <b id='7'>7 <span style='color:#ffa64d'>|</span> Building the Models.</b>

<div id='7.1' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>7.1 | </span></span></b>Confusion Matrix drawing function.</b></p></div>
"""

def draw_confusion_matrix(y_test,test_predict):
    confusion_matrix = metrics.confusion_matrix(y_test, test_predict)

    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

    cm_display.plot()
    plt.show()

"""<div id='7.2' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>7.2 | </span></span></b>Decision Tree.</b></p></div>

Decision Tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.

There are several hyperparameters in the Decision Tree object from Scikit-learn.

The hyperparameters we will use and investigate here are:

<span style='color:#ffa64d'>criterion: </span>The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain.

<span style='color:#ffa64d'>min_samples_split: </span>The minimum number of samples required to split an internal node.
Choosing a higher min_samples_split can reduce the number of splits and may help to reduce overfitting.

<span style='color:#ffa64d'>max_depth: </span>The maximum depth of the tree.
Choosing a lower max_depth can reduce the number of splits and may help to reduce overfitting.
"""

model=DecisionTreeClassifier(random_state=55)

"""<span style='color:#ffa64d'>Find Best Parameter With Gridsearchcv</span>"""

search_space_dtc = {
    'criterion' : ['gini', 'entropy'],
    'max_depth': [1,2, 3, 4, 8, 16, 32, 64, None] ,
    'min_samples_split' : [2,10, 30, 50, 100, 200, 300, 700] 
}

gs = GridSearchCV(estimator=model,
                  param_grid=search_space_dtc,
                  scoring = ['r2','neg_root_mean_squared_error'],
                  refit = 'r2',
                  cv=5,
                
                 )
gs.fit(X_train,y_train)

gs.best_params_

X_train_prediction = gs.best_estimator_.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
print('Accuracy on Training data : ', training_data_accuracy)
    
X_test_prediction = gs.best_estimator_.predict(X_val)
test_data_accuracy = accuracy_score(X_test_prediction, y_val)
print('Accuracy score on Test Data : ', test_data_accuracy)

draw_confusion_matrix(y_val,X_test_prediction)

precision_recall_fscore_support(y_val, X_test_prediction, average='macro')

"""<div id='7.3' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>7.3 | </span></span></b>Random Forest.</b></p></div>

The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.

Now let's try the Random Forest algorithm also, using the Scikit-learn implementation.

All of the hyperparameters found in the decision tree model will also exist in this algorithm, since a random forest is an ensemble of many Decision Trees.

One additional hyperparameter for Random Forest is called n_estimators which is the number of Decision Trees that make up the Random Forest.
"""

random_forest_model=RandomForestClassifier()

"""<span style='color:#ffa64d'>Find Best Parameter With Gridsearchcv</span>"""

search_space_rfc = {
    'n_estimators' : [100,200,300],
    'max_depth': [2,3,4,5,6],
    'min_samples_split' : [2,10, 30, 50] 
}

gs_rfc = GridSearchCV(estimator=random_forest_model,
                  param_grid=search_space_rfc,
                  scoring = ['r2','neg_root_mean_squared_error'],
                  refit = 'r2',
                  cv=5,
                
                 )
gs_rfc.fit(X_train,y_train)

gs_rfc.best_params_

X_train_prediction = gs_rfc.best_estimator_.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
print('Accuracy on Training data : ', training_data_accuracy)
    
X_test_prediction = gs_rfc.best_estimator_.predict(X_val)
test_data_accuracy = accuracy_score(X_test_prediction, y_val)
print('Accuracy score on Test Data : ', test_data_accuracy)

draw_confusion_matrix(y_val,X_test_prediction)

precision_recall_fscore_support(y_val, X_test_prediction, average='macro')

"""<div id='7.4' style="color:white;display:fill;border-radius:8px;background-color:#323232;font-size:150%; letter-spacing:1.0px"><p style="padding: 12px;color:white;"><b><b><span style='color:white'><span style='color:#ffa64d'>7.3 | </span></span></b>XGBoost.</b></p></div>

Next is the Gradient Boosting model, called XGBoost. The boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error.

The model has the same parameters as a decision tree, plus the learning rate.

The learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.

One interesting thing about the XGBoost is that during fitting, it can take in an evaluation dataset of the form (X_val,y_val).
"""

from xgboost import XGBClassifier
xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = 55)
xgb_model.fit(X_train,y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 10)

"""Even though we initialized the model to allow up to 500 estimators, the algorithm only fit 45 estimators.

To see why, let's look for the round of training that had the best performance (lowest evaluation metric). We can either view the validation log loss metrics that were output above, or view the model's .best_iteration attribute:
"""

xgb_model.best_iteration

"""The best round of training was round 35, with a log loss of 0.30799."""

X_train_prediction = xgb_model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
print('Accuracy on Training data : ', training_data_accuracy)
    
X_test_prediction = xgb_model.predict(X_val)
test_data_accuracy = accuracy_score(X_test_prediction, y_val)
print('Accuracy score on Test Data : ', test_data_accuracy)

draw_confusion_matrix(y_val,X_test_prediction)

precision_recall_fscore_support(y_val, X_test_prediction, average='macro')

def model(classifier):
    
    classifier.fit(X_train,y_train)
    prediction = classifier.predict(X_val)
    cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
    print("Accuracy : ",'{0:.2%}'.format(accuracy_score(y_val,prediction)))
    print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(classifier,X_train,y_train,cv = cv,scoring = 'roc_auc').mean()))
    print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_val,prediction)))
    RocCurveDisplay.from_estimator(classifier, X_val, y_val)
    plt.title('ROC_AUC_Plot')
    plt.show()

def model_evaluation(classifier):
    
    # Confusion Matrix
    cm = confusion_matrix(y_val,classifier.predict(X_val))
    names = ['True Neg','False Pos','False Neg','True Pos']
    counts = [value for value in cm.flatten()]
    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
    labels = np.asarray(labels).reshape(2,2)
    sns.heatmap(cm,annot = labels,cmap = color,fmt ='')
    
    # Classification Report
    print(classification_report(y_val,classifier.predict(X_val)))

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import precision_recall_curve

from sklearn.neighbors import KNeighborsClassifier
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_val)
    mean_acc[n-1] = metrics.accuracy_score(y_val, yhat)

    
    std_acc[n-1]=np.std(yhat==y_val)/np.sqrt(yhat.shape[0])

mean_acc

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="#0080FF")
plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()

KNNclassifier= KNeighborsClassifier(leaf_size = 1, n_neighbors = 9, p = 1)
model(KNNclassifier)
model_evaluation(KNNclassifier)

from sklearn.svm import SVC
classifierSVC= SVC(kernel = 'linear', C= 0.1)
model(classifierSVC)
model_evaluation(classifierSVC)

from sklearn.linear_model import LogisticRegression
classifier_lr = LogisticRegression(random_state=0, C= 10, penalty= 'l2', max_iter =600)
model(classifier_lr)
model_evaluation(classifier_lr)

import pickle
pickle.dump(gs_rfc,open('model.pkl','wb'))
model=pickle.load(open('model.pkl','rb'))

data = np.array([[40,140,289,0,172,0.0,0,0,1,0,0,0,0,1,0,1,0,0,0,1]])
output = model.predict(data)
print(output[0])